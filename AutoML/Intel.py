# -*- coding: utf-8 -*-
"""Training_3 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kddM2zBnymvv0EKkBbt8FkJr7GyBMuDr
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import os
import kagglehub
from keras.models import load_model
from tensorflow.keras.preprocessing import image
import json
from google.colab import files
import random
from PIL import Image
import pandas as pd

path = kagglehub.dataset_download("puneet6060/intel-image-classification")

print("Path to dataset files:", path)

# Correct the TRAIN_DIR and TEST_DIR paths.
# The dataset was downloaded to 'path', and the subdirectories are 'seg_train/seg_train' and 'seg_test/seg_test'.
TRAIN_DIR = os.path.join(path, 'seg_train', 'seg_train')
TEST_DIR = os.path.join(path, 'seg_test', 'seg_test')

widths = []
heights = []

for cls in classes:
    class_path = os.path.join(TRAIN_DIR, cls)
    for img_name in os.listdir(class_path)[:100]:  # sample only
        img = Image.open(os.path.join(class_path, img_name))
        w, h = img.size
        widths.append(w)
        heights.append(h)

plt.figure(figsize=(8,5))
plt.scatter(widths, heights, alpha=0.5)
plt.xlabel("Width")
plt.ylabel("Height")
plt.title("Original Image Size Distribution")
plt.grid(True)
plt.show()

IMG_SIZE = (150, 150)
BATCH_SIZE = 32
NUM_CLASSES = 6

classes = os.listdir(TRAIN_DIR)

plt.figure(figsize=(12, 8))

for i, cls in enumerate(classes):
    class_path = os.path.join(TRAIN_DIR, cls)
    img_name = random.choice(os.listdir(class_path))
    img_path = os.path.join(class_path, img_name)

    img = Image.open(img_path)

    plt.subplot(2, 3, i+1)
    plt.imshow(img)
    plt.title(cls)
    plt.axis("off")

plt.suptitle("Sample Images from Each Class (Raw Data)", fontsize=16)
plt.show()

class_counts = {}

for cls in classes:
    class_counts[cls] = len(os.listdir(os.path.join(TRAIN_DIR, cls)))

df = pd.DataFrame.from_dict(class_counts, orient='index', columns=['Count'])

plt.figure(figsize=(8,5))
df['Count'].plot(kind='bar')
plt.title("Number of Images per Class")
plt.xlabel("Class")
plt.ylabel("Number of Images")
plt.grid(axis='y')
plt.show()

"""# **CNN Model**"""

# ======================
# Data Augmentation
# ======================
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    # zoom_range=0.3,
    zoom_range=0.35,
    shear_range=0.15,
    # brightness_range=[0.8, 1.2],
    brightness_range=[0.7, 1.3],
    horizontal_flip=True,
    fill_mode='nearest'
)

train_data = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(150,150),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

val_data = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(150,150),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

print("Class Indices:", train_data.class_indices)

img = raw_img_array.reshape((1,) + raw_img_array.shape)

augmented = next(aug_gen.flow(img))[0]

plt.figure(figsize=(8,4))

plt.subplot(1,2,1)
plt.imshow(raw_img_array.astype("uint8"))
plt.title("Original Image")
plt.axis("off")

plt.subplot(1,2,2)
plt.imshow(augmented)
plt.title("After Augmentation")
plt.axis("off")

plt.show()

images, labels = next(train_data)

plt.figure(figsize=(10,10))

for i in range(9):
    plt.subplot(3,3,i+1)
    plt.imshow(images[i])
    plt.title(f"Augmented Image {i+1}")
    plt.axis("off")

plt.tight_layout()
plt.show()

plt.figure(figsize=(12,6))

for i in range(6):
    augmented = next(aug_gen.flow(img))[0]
    plt.subplot(2,3,i+1)
    plt.imshow(augmented)
    plt.axis("off")

plt.suptitle("Different Augmentations of the Same Image", fontsize=14)
plt.show()

# ======================
# CNN Model (Optimized)
# ======================
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(150,150,3)),

    # ===== Block 1 =====
    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(2,2),

    # ===== Block 2 =====
    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(2,2),

    # ===== Block 3 =====
    tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(2,2),

    # ===== Block 4 (New – Deeper Features) =====
    tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu'),
    tf.keras.layers.BatchNormalization(),

    tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(2,2),

    # ===== Global Pooling =====
    tf.keras.layers.GlobalAveragePooling2D(),

    # ===== Classifier =====
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.5),

    tf.keras.layers.Dense(6, activation='softmax')
])

# ======================
# Compile (with Label Smoothing)
# ======================
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),
    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
    metrics=['accuracy']
)

model.summary()

# ======================
# Callbacks
# ======================
early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=7,
    restore_best_weights=True
)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.3,
    patience=4,
    min_lr=1e-6,
    verbose=1
)

# ======================
# Training (Stage 1)
# ======================
history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=50,
    callbacks=[early_stop, reduce_lr]
)

# ======================
# Fine Tuning (Stage 2)
# ======================
for layer in model.layers[:-8]:
    layer.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
    metrics=['accuracy']
)

history_finetune = model.fit(
    train_data,
    validation_data=val_data,
    epochs=15,
    callbacks=[early_stop, reduce_lr]
)

# ======================
# Testing
# ======================
test_datagen = ImageDataGenerator(rescale=1./255)

test_data = test_datagen.flow_from_directory(
    TEST_DIR,
    target_size=(150,150),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)
test_loss, test_acc = model.evaluate(test_data)
print("Test Accuracy:", test_acc)

# ======================
# Evaluation Metrics
# ======================
y_true = test_data.classes
y_pred = model.predict(test_data)
y_pred_classes = np.argmax(y_pred, axis=1)

print(classification_report(
    y_true,
    y_pred_classes,
    target_names=test_data.class_indices.keys()
))

# ======================
# Confusion Matrix
# ======================
cm = confusion_matrix(y_true, y_pred_classes)

plt.figure(figsize=(7,6))
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=test_data.class_indices.keys(),
    yticklabels=test_data.class_indices.keys()
)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

# ======================
# Accuracy Curves
# ======================
plt.figure(figsize=(8,5))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

model.save('intel_image_classification_model.keras')
print("Model saved as 'intel_image_classification_model.keras'")

model.save('intel_image_classification_model.h5', save_format='h5')
print("Model saved successfully as .h5")

files.download('intel_image_classification_model.h5')

print(os.getcwd())
print(os.listdir())

model = load_model(
    "/content/intel_image_classification_model.keras",
    safe_mode=False
)

img = image.load_img(
    "10047.jpg",
    target_size=(150,150)
)

img_array = image.img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)

prediction = model.predict(img_array)
class_index = np.argmax(prediction)

print("Predicted Class:", class_index)

class_names = list(train_data.class_indices.keys())
print("Predicted Class:", class_names[class_index])

cm = confusion_matrix(y_true, y_pred)

cnn_results = {
    "CNN": 0.89
}

cnn_conf = {
    "CNN": cm.tolist()
}

with open("results_CNN.json", "w") as f:
    json.dump({"Image Classification": cnn_results}, f)

with open("results_CNN_conf.json", "w") as f:
    json.dump({"Image Classification": cnn_conf}, f)

"""
# **Resnet50**"""

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D

resnet_train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    validation_split=0.2,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.35,
    shear_range=0.15,
    brightness_range=[0.7, 1.3],
    horizontal_flip=True,
    fill_mode='nearest'
)

resnet_train_data = resnet_train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

resnet_val_data = resnet_train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

images, labels = next(resnet_train_data)

plt.figure(figsize=(10,5))
for i in range(5):
    plt.subplot(1,5,i+1)
    plt.imshow(images[i])
    plt.axis("off")
plt.suptitle("Augmented Images")
plt.show()

base_model = ResNet50(
    weights='imagenet',
    include_top=False,
    input_shape=(224,224,3)
)

# Freeze base model
base_model.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
output = Dense(6, activation='softmax')(x)

resnet_model = Model(inputs=base_model.input, outputs=output)

resnet_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
    metrics=['accuracy']
)

resnet_model.summary()

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=7,
    restore_best_weights=True
)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.3,
    patience=4,
    min_lr=1e-6,
    verbose=1
)

history_resnet = resnet_model.fit(
    resnet_train_data,
    validation_data=resnet_val_data,
    epochs=25,
    callbacks=[early_stop, reduce_lr]
)

# Unfreeze last 30 layers
for layer in base_model.layers[-30:]:
    layer.trainable = True

resnet_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),
    metrics=['accuracy']
)

history_resnet_finetune = resnet_model.fit(
    resnet_train_data,
    validation_data=resnet_val_data,
    epochs=15,
    callbacks=[early_stop, reduce_lr]
)

resnet_test_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input
)

resnet_test_data = resnet_test_datagen.flow_from_directory(
    TEST_DIR,
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

resnet_test_loss, resnet_test_acc = resnet_model.evaluate(resnet_test_data)
print("ResNet50 Test Accuracy:", resnet_test_acc)

y_true_resnet = resnet_test_data.classes
y_pred_resnet = resnet_model.predict(resnet_test_data)
y_pred_resnet_classes = np.argmax(y_pred_resnet, axis=1)

print(classification_report(
    y_true_resnet,
    y_pred_resnet_classes,
    target_names=resnet_test_data.class_indices.keys()
))

cm_resnet = confusion_matrix(y_true_resnet, y_pred_resnet_classes)

plt.figure(figsize=(7,6))
sns.heatmap(
    cm_resnet,
    annot=True,
    fmt='d',
    cmap='Greens',
    xticklabels=resnet_test_data.class_indices.keys(),
    yticklabels=resnet_test_data.class_indices.keys()
)
plt.title("ResNet50 Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# ======================
# Accuracy Curves for ResNet50
# ======================

# Combine history from both training stages of ResNet50
combined_resnet_accuracy = history_resnet.history['accuracy'] + history_resnet_finetune.history['accuracy']
combined_resnet_val_accuracy = history_resnet.history['val_accuracy'] + history_resnet_finetune.history['val_accuracy']

plt.figure(figsize=(10,6))
plt.plot(combined_resnet_accuracy, label='ResNet50 Train Accuracy')
plt.plot(combined_resnet_val_accuracy, label='ResNet50 Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('ResNet50 Training and Validation Accuracy')
plt.legend()
plt.grid(True)
plt.show()

resnet_model.save("intel_resnet50_model.keras")
print("ResNet50 model saved successfully")

files.download('intel_resnet50_model.keras')

resnet_model.save("intel_resnet50_model.h5")
print("ResNet50 model saved successfully")

files.download('intel_resnet50_model.h5')

# ======================
# Accuracy JSON
# ======================
accuracy_data = {
    "Intel Image Classification": {
        "ResNet50": float(resnet_test_acc)
    }
}

# ======================
# Confusion Matrix JSON
# ======================
conf_matrix_data = {
    "Intel Image Classification": {
        "ResNet50": cm_resnet.tolist()
    }
}

# ======================
# Save JSON files
# ======================
with open("results_intel.json", "w") as f:
    json.dump(accuracy_data, f, indent=4)

with open("confs_intel.json", "w") as f:
    json.dump(conf_matrix_data, f, indent=4)

print("✅ Intel Image JSON files saved successfully")